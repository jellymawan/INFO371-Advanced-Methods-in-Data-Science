{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ec47ea-a6a4-45c2-8611-06a6956089c8",
   "metadata": {},
   "source": [
    "# 1. (7pt) Load and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c4b00-cf96-47ac-bbea-c1903be3fbcc",
   "metadata": {},
   "source": [
    "### 1. (1pt) Load and clean data. Feel free to copy-paste from your PS05 solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34523a15-c503-4e54-bafd-18f07c34ba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam       0\n",
       "files      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "emails = pd.read_csv(\"../INFO-371/lingspam-emails.csv.bz2\", sep=\"\\t\")\n",
    "np.shape(emails)\n",
    "#pd.set_option(\"display.max_colwidth\", -1) # uncomment to see all the text\n",
    "\n",
    "df = emails.dropna(subset=[\"spam\", \"message\"])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbf8ea-4e93-4ef6-8a29-a201568de62e",
   "metadata": {},
   "source": [
    "### 2. (2pt) Vectorize emails so you have a DTM (I’ll refer to this as the design matrix X) and the spam/non-spam indicator y. How many different documents (emails) and different tokens (words) do you have in these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f448e582-c586-4a07-9e6e-796995db6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "# define vectorizer\n",
    "X = vectorizer.fit_transform(emails.message)\n",
    "# vectorize your data. Note: this creates a sparse matrix,\n",
    "# use .toarray() if you run into trouble\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "# in case you want to see what are the actual words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083a4e4-cc55-4618-ab33-515e31d2608c",
   "metadata": {},
   "source": [
    "### 3. (2pt) Split data into training/validation chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b0908c-b23e-4058-a8e4-8607c7493309",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.spam * 1\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed537666-9fd1-4f13-b9a7-3e3d00788346",
   "metadata": {},
   "source": [
    "### 4. (2pt) Design a scheme to name your variables so you can understand (and you grader can understand too!) which mathematical concept it refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f035463-4424-4799-8764-739edaa70b13",
   "metadata": {},
   "source": [
    "* P_S1: probability of spam. (Pr(S = 1))\n",
    "* P_S0: probability of not spam. (Pr(S = 0))\n",
    "* S0_W: probability the email is not spam given it cointains the word W. (Pr(S = 0|W = 1))\n",
    "* S1_W: probability the email is spam given it contains the word W. (Pr(S = 1|W = 1))\n",
    "* W1_S1: probability the word is present, given the email is spam. (Pr(W = 1|S = 1))\n",
    "* W1_S0: probability the word is present, given the email is non-spam. (Pr(W = 1|S = 0))\n",
    "\n",
    "\n",
    "* log_W1_S1: log probability that the word is present, given the email is spam. (log Pr(W = 1|S = 1))\n",
    "* log_W1_S0: log probability that the word is present, given the email is non-spam. (log Pr(W = 1|S = 0))\n",
    "* log_S1_W: log likelihood of email being spam, given it contains the word W\n",
    "* log_S0_W: log likelihood of email being non-spam, given it contains the word W\n",
    "* log_S1_W_sm and log_S0_W_sm: same as the above 2, but with smoothing\n",
    "* log_S1: log probability of spam (log Pr(S = 1))\n",
    "* log_S0: log probability of non-spam (log Pr(S = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2371e-bb85-458d-b3c9-2594823aa152",
   "metadata": {},
   "source": [
    "# 2. (42pt) Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97394bdf-360d-4127-a57e-c04d88677094",
   "metadata": {},
   "source": [
    "### 1. (4pt) Here is a small excerpt from the initial DTM (before you split it into training/validation), corresponding to rows 983 to 985, and to columns 40,042–40,046:\n",
    "```\n",
    "X[982:985, 40041:40046].toarray()\n",
    "## array([[0, 0, 0, 0, 0],\n",
    "##        [0, 1, 0, 1, 0],\n",
    "##        [0, 0, 0, 0, 0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a8d22-2c3e-425f-a363-58118e2b1878",
   "metadata": {},
   "source": [
    "### What do these numbers show:\n",
    "#### (a) which emails do the rows correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f78f489-a29b-4b94-8d54-02a0f91f89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>files</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>False</td>\n",
       "      <td>6-1148msg1.txt</td>\n",
       "      <td>Subject: summary : parsing of ambiguous sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>False</td>\n",
       "      <td>6-1149msg1.txt</td>\n",
       "      <td>Subject: re : sapir - whorf and what to tell s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>False</td>\n",
       "      <td>6-1150msg1.txt</td>\n",
       "      <td>Subject: call for contributions  call for cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam           files                                            message\n",
       "982  False  6-1148msg1.txt  Subject: summary : parsing of ambiguous sequen...\n",
       "983  False  6-1149msg1.txt  Subject: re : sapir - whorf and what to tell s...\n",
       "984  False  6-1150msg1.txt  Subject: call for contributions  call for cont..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[982:985]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f9962-6c43-428a-a3ea-1c787bbba431",
   "metadata": {},
   "source": [
    "#### (b) Which words do the columns correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadffe81-1d6b-4b1d-a420-f82fa4266a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nooteboom', 'nootka', 'nope', 'nor', 'nora'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[40041:40046]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470da86-0b1f-4de8-a3c3-cf22de80802c",
   "metadata": {},
   "source": [
    "#### (c) What do the “1”-s in the middle of the table mean? \n",
    "The 1's in the middle of the table means that the specific word exists in the email.\n",
    "#### (d) What do the zeros mean?\n",
    "The 0's means that the specific word does not exist in the email\n",
    "### Note: you should have exactly the same numbers in your analysis, this is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39910e26-0a77-48f1-a404-33b5e7e2cc59",
   "metadata": {},
   "source": [
    "### 2. (2pt) What is the accuracy of the naive model that predicts all emails into the majority category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c7641b-eb6c-4223-af7c-de8000bf5339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regular emails: 2412\n",
      "Number of spam emails: 481\n",
      "Accuracy: 0.8337366055997235\n"
     ]
    }
   ],
   "source": [
    "count_reg = y[y == False].shape[0]\n",
    "print(\"Number of regular emails: \" + str(count_reg))\n",
    "count_spam = y[y == 1].shape[0]\n",
    "print(\"Number of spam emails: \" + str(count_spam))\n",
    "\n",
    "print(\"Accuracy: \" + str(count_reg / (count_spam + count_reg)))\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945d1c7-f724-405e-a008-161dcffa6b6e",
   "metadata": {},
   "source": [
    "### 3. (3pt) Compute the unconditional (log) probability that the email is spam/non-spam, log Pr(S = 1), and log Pr(S = 0). These probabilities are based on the values of y (i.e. spam) alone. They do not contain information about the words in emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd92d539-1e40-485a-80ae-88f44e65f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Pr(S = 1): -1.7678471428525\n",
      "log Pr(S = 0): -0.18717341174592203\n"
     ]
    }
   ],
   "source": [
    "P_S1 = np.mean(train_y)\n",
    "P_S0 = 1 - P_S1\n",
    "\n",
    "log_S1 = np.log(P_S1)\n",
    "print(\"log Pr(S = 1):\", log_S1)\n",
    "log_S0 = np.log(P_S0)\n",
    "print(\"log Pr(S = 0):\", log_S0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8536c4-c51a-4e4b-b67c-6db360d05997",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.  For each word w, compute the (log) probability that the word is present in spam emails, log Pr(W = 1|S = 1), and (log) probability that the word is present in non-spam emails, log Pr(W = 1|S = 0). These probabilities can easily be calculated from counts of how many times these words are present for each class.\n",
    "\n",
    "Hint: these computations are based on your BOW-s X. Look at ways to sum along columns in this matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edea42f-e55f-41cb-affd-6ed2469f9c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_310/909295264.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  log_W1_S1 = np.log(W1_S1)\n",
      "/tmp/ipykernel_310/909295264.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  log_W1_S0 = np.log(W1_S0)\n"
     ]
    }
   ],
   "source": [
    "W1_S1 = train_X[train_y == 1,].mean(axis = 0)\n",
    "log_W1_S1 = np.log(W1_S1)\n",
    "\n",
    "W1_S0 = train_X[train_y == 0,].mean(axis = 0)\n",
    "log_W1_S0 = np.log(W1_S0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5e6b7-d632-440e-a2d4-e9104b9cde10",
   "metadata": {},
   "source": [
    "### 5. What should be the dimension of your log Pr(W = 1|S = 0) and log Pr(W = 1|S = 1) vectors? Explain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57e95efe-2fe3-4a24-a58c-134611c720f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_W1_S1 shape (1, 60925) log_W1_S0 shape (1, 60925)\n"
     ]
    }
   ],
   "source": [
    "print(\"log_W1_S1 shape\", log_W1_S1.shape, \"log_W1_S0 shape\", log_W1_S0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed54226-18bd-4f09-a7b3-12aade65c297",
   "metadata": {},
   "source": [
    "Both should have the same shape, and both should also have equal length as the number of unique words in all the emails. This is because we are trying to determine the (log) probability of each word appears in emails that are classified as spam and emails that are classified as non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae819a-0dae-4642-96f1-2b8b9c4bdab3",
   "metadata": {},
   "source": [
    "### 6. For both classes, S = 1 and S = 0, compute the log-likelihood that the email belongs to this class. Log-likelihood is given as (7.3.20 and 7.3.21, page 270 for now) in lecture notes, and the equations in Schutt “Doing Data Science”, page 102.\n",
    "\n",
    "#### Computing the likelihoods involves sums of the previously computed probabilities, log Pr(W = 1|S), and BOW elements xij. Start by doing this by whatever way you can get it done (e.g. loops). The most important thing is that you understand what you do!\n",
    "\n",
    "#### But if you want to write efficient code, use matrix product instead (it is ∼ 1000× faster than loops). See Lecture Notes (7.30.30) for how to do it with matrix product. You can also check out np.apply_along_axis as an alternative way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef16224-eabc-42d6-ba2f-4d477b636d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((579, 1), (579, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_S1_W = log_S1 + test_X @ np.transpose(log_W1_S1)\n",
    "\n",
    "log_S0_W = log_S0 + test_X @ np.transpose(log_W1_S0)\n",
    "\n",
    "log_S1_W.shape, log_S0_W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4e3ce-ee18-4ff2-90cc-0a09e3b6974b",
   "metadata": {},
   "source": [
    "### 7. How many log-likelihoods you have to compute? Explain why do you have to have this many log-likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940f321-bf2a-4092-98ce-f5364a021751",
   "metadata": {},
   "source": [
    "We have to compute 2K likelihoods, where K is the amount of emails. Since we are trying to determine whether or not an email is classified as spam or non-spam, we have to compute the probability that it will be spam and non-spam and pick the higher value. So for one email, we have to compute its spam and non-spam probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f90ee-1a63-4bec-b89d-6d9fc71c116c",
   "metadata": {},
   "source": [
    "### 8. Based on the log-likelihoods, predict the class S = 1 or S = 0 for each email in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93b9c04-b30c-48d0-a426-fc17486aa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = (log_S1_W > log_S0_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e09ef9-8214-46bb-a3a0-0e8a00a74f77",
   "metadata": {},
   "source": [
    "### 9. Print the resulting confusion matrix and accuracy (feel free to use existing libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25cf6cdc-0214-4ab5-b4d9-050a6bc1b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[493   0]\n",
      " [ 75  11]]\n",
      "accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_y, compare)\n",
    "print(cm)\n",
    "print(\"accuracy:\", str(round((cm[0,0]+cm[1,1])/(np.sum(cm)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b50c7-258f-4990-9c85-5af2af16c6ef",
   "metadata": {},
   "source": [
    "### 10. If your results are like mine, you can see that the results are not impressive at all, your model works no better than the naive guess. Explain why do you get such mediocre results.\n",
    "\n",
    "#### Hint: this is related to infinites, where are those coming from, and why they make the model useless? See also the smoothing-related discussion in Lecture Notes at the end of the Naive Bayes (Section 7.3.3), before Example 7.3.\n",
    "\n",
    "#### Note: just explain, but do not do anything about it! We’ll attack the problem in the next question with smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a86a17-45e2-4c3e-ac24-d6d9a99fb412",
   "metadata": {},
   "source": [
    "Our accuracy was not exactly the same as the naive model, but it is still pretty similar. \n",
    "\n",
    "If there is an email that is supposed to be spam, but one of the words in the email has not yet appeared in the spam (ex: Pr(S|W = 0) = 0), this email will never be classified as spam, even though it clearly should be because 0 multiplied by anything will always be 0. For example, if a spam email containing the word 'viagra' appears and Pr(S|viagra = 0) = 0, it will not be classified as spam because 0 multiplied by anything will always be 0. Hence, since we always classify the email class according to the highest probability, it will not be counted as spam, even when it clearly is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63bcba6-7d79-4c1d-8129-a993bdb987b0",
   "metadata": {},
   "source": [
    "## 3. Add smoothing\n",
    "\n",
    "### 1. As you will be doing validation below, your first task is to mold what you did above into two functions: one for fitting and another one for predicting. You can mostly copy-paste your code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ef4fcd-a28d-46de-8860-e4d5e3166ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data_orig(alpha):\n",
    "    a = alpha\n",
    "    P_S1 = np.mean(train_y)\n",
    "    P_S0 = 1 - P_S1\n",
    "    log_S1 = np.log10(P_S1)\n",
    "    log_S0 = np.log10(P_S0)\n",
    "    \n",
    "    W1_S1 = train_X[train_y == 1,].mean(axis = 0)\n",
    "    log_W1_S1 = np.log10(W1_S1)\n",
    "\n",
    "    W1_S0 = train_X[train_y == 0,].mean(axis = 0)\n",
    "    log_W1_S0 = np.log10(W1_S0)\n",
    "    \n",
    "    log_S1_W = log_S1 + test_X @ np.transpose(log_W1_S1)\n",
    "    log_S0_W = log_S0 + test_X @ np.transpose(log_W1_S0)\n",
    "    \n",
    "    return log_S1_W, log_S0_W\n",
    "    \n",
    "\n",
    "def pred_data_orig(log_S1_W, log_S0_W):\n",
    "    compare = (log_S1_W > log_S0_W)\n",
    "    cm = confusion_matrix(test_y, compare)\n",
    "    print(cm)\n",
    "    print(str(round((cm[0,0]+cm[1,1])/(np.sum(cm)), 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70d46f-6ef5-4fc6-bfcf-d8733645eafb",
   "metadata": {},
   "source": [
    "### 2. Add smoothing to the model. Smoothing amounts to assuming that we have “seen” every possible word α ⩾ 0 times already, in both spam and non-spam emails. Note that α does not have to be an integer, and typically the best α < 1.\n",
    "\n",
    "#### What you have to do is to re-compute the probabilities log Pr(S = 1), log Pr(S = 0), log Pr(W = 1|S = 1), log Pr(W = 1|S = 1), the predictions part will remain unchanged. So you should update your fitting function by adding an additional argument α to it, and modify the probabilities accordingly. (And you use only training data for this.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847b24fd-8f38-493f-9128-184a500d4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(alpha):\n",
    "    a = alpha\n",
    "    P_S1 = np.mean(train_y)\n",
    "    P_S0 = 1 - P_S1\n",
    "    log_S1 = np.log(P_S1)\n",
    "    log_S0 = np.log(P_S0)\n",
    "\n",
    "    tot_S1 = (train_X[train_y == 1]).shape[0]\n",
    "    W1_S1 = (np.sum(train_X[train_y == 1], axis = 0) + alpha)  / (tot_S1 + 2*alpha)\n",
    "    log_W1_S1 = np.log(W1_S1)\n",
    "\n",
    "    tot_S0 = (train_X[train_y == 0]).shape[0]\n",
    "    W1_S0 = (np.sum(train_X[train_y == 0], axis = 0)+ alpha)  / (tot_S0 + 2*alpha)\n",
    "    log_W1_S0 = np.log(W1_S0)\n",
    "\n",
    "    log_S1_W = log_S1 + test_X @ np.transpose(log_W1_S1)\n",
    "    log_S0_W = log_S0 + test_X @ np.transpose(log_W1_S0)\n",
    "\n",
    "    return log_S1_W, log_S0_W, a\n",
    "    \n",
    "\n",
    "def pred_data(log_S1_W, log_S0_W, alpha):\n",
    "    print(\"alpha:\", alpha)\n",
    "    compare = (log_S1_W > log_S0_W)\n",
    "    cm = confusion_matrix(test_y, compare)\n",
    "    print(cm)\n",
    "    acc = round((cm[0,0]+cm[1,1])/(np.sum(cm)), 3)\n",
    "    print(\"accuracy:\", acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e37b84-9aa6-4a21-8fdf-7b1e5fc51be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "alpha: 0.2\n",
      "[[478  15]\n",
      " [  1  85]]\n",
      "accuracy: 0.972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.972"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test:\")\n",
    "\n",
    "log_S1_W_sm, log_S0_W_sm, alpha = fit_data(0.2)\n",
    "\n",
    "pred_data(log_S1_W_sm, log_S0_W_sm, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45074547-a68d-4973-b26a-df6af116605e",
   "metadata": {},
   "source": [
    "### 3. Use your updated model for predictions with a few different α-values (on validation data) and report the corresponding confusion matrix and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067b4e4c-0169-4665-80e9-109a4de50bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.9\n",
      "[[435  58]\n",
      " [  0  86]]\n",
      "accuracy: 0.9\n",
      "alpha: 0.5\n",
      "[[466  27]\n",
      " [  0  86]]\n",
      "accuracy: 0.953\n",
      "alpha: 0.2\n",
      "[[478  15]\n",
      " [  1  85]]\n",
      "accuracy: 0.972\n",
      "alpha: 0.01\n",
      "[[491   2]\n",
      " [  2  84]]\n",
      "accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "log_S1_W_sm, log_S0_W_sm, alpha = fit_data(0.9)\n",
    "\n",
    "pred_data(log_S1_W_sm, log_S0_W_sm, alpha)\n",
    "\n",
    "log_S1_W_sm, log_S0_W_sm, alpha = fit_data(0.5)\n",
    "\n",
    "pred_data(log_S1_W_sm, log_S0_W_sm, alpha)\n",
    "\n",
    "log_S1_W_sm, log_S0_W_sm, alpha = fit_data(0.2)\n",
    "\n",
    "pred_data(log_S1_W_sm, log_S0_W_sm, alpha)\n",
    "\n",
    "log_S1_W_sm, log_S0_W_sm, alpha = fit_data(0.01)\n",
    "\n",
    "curr = pred_data(log_S1_W_sm, log_S0_W_sm, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbac737-a5c7-452e-ac46-721b6b980a02",
   "metadata": {},
   "source": [
    "### 4. Use validation to find the best smoothing parameter α. You can just run a loop over different values, but start with very small values (10−8, 10−7, 10−6 up to perhaps 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "370f1131-29a9-4253-9de6-a5d2976c1b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n",
      "[[486   7]\n",
      " [  2  84]]\n",
      "accuracy: 0.984\n",
      "alpha: 0.01\n",
      "[[491   2]\n",
      " [  2  84]]\n",
      "accuracy: 0.993\n",
      "alpha: 0.001\n",
      "[[491   2]\n",
      " [  3  83]]\n",
      "accuracy: 0.991\n",
      "alpha: 0.0001\n",
      "[[491   2]\n",
      " [  4  82]]\n",
      "accuracy: 0.99\n",
      "alpha: 1e-05\n",
      "[[492   1]\n",
      " [  5  81]]\n",
      "accuracy: 0.99\n",
      "alpha: 1e-06\n",
      "[[492   1]\n",
      " [  6  80]]\n",
      "accuracy: 0.988\n",
      "alpha: 1e-07\n",
      "[[492   1]\n",
      " [  8  78]]\n",
      "accuracy: 0.984\n",
      "alpha: 1e-08\n",
      "[[492   1]\n",
      " [  8  78]]\n",
      "accuracy: 0.984\n",
      "alpha: 1e-09\n",
      "[[492   1]\n",
      " [  9  77]]\n",
      "accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for exp in range(1, 10):\n",
    "    log_S1_W_sm, log_S0_W_sm, alpha = fit_data(10**(-exp))\n",
    "    acc.append(pred_data(log_S1_W_sm, log_S0_W_sm, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffff4b-8469-481b-9023-f86808d541e8",
   "metadata": {},
   "source": [
    "### 5. Plot how accuracy depends on α. Use log-scale for α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e09348-06d0-4653-bad0-74b7a649a1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = []\n",
    "for exp in range(1, 10):\n",
    "    alpha.append(10**(-exp))\n",
    "    \n",
    "plt.scatter(np.log(alpha), acc)\n",
    "plt.xlabel(\"Alpha (ln(alpha))\")\n",
    "plt.ylabel(\"Accuracy (out of 0-1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ffdde-8d9e-4351-89fd-d2139f8b2a6e",
   "metadata": {},
   "source": [
    "## 4. Interpretation\n",
    "\n",
    "### 1. Which words are the best predictors that an email is spam? These are the word where Pr(S = 1|W = 1) is large and Pr(S = 0|W = 1) is small, or to put it differently, where log Pr(S = 1|W = 1) − log Pr(S = 0|W = 1) is large.\n",
    "\n",
    "#### Explain why this is the case.\n",
    "#### Hint: you may re-check the concept of log-likelihood and how that is used for prediction.\n",
    "#### Hint 2: you may imagine you receive 60k 1-word emails (one for each word in your vocabulary). Which ones are most likely spam, and which ones are least likely spam?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b60c6e-e00a-4180-8e95-502f367f25e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1302' 'advertiser' 'bonus' 'capitalfm' 'debts' 'earning' 'earnings'\n",
      " 'fabulous' 'fantastic' 'gambling' 'hottest' 'instant' 'intrusion'\n",
      " 'lottery' 'mlm' 'mortgage' 'privacy' 'profits' 'relax' 'resell' 'retire'\n",
      " 'sexually' 'spam' 'stealth' 'unsubscribe']\n"
     ]
    }
   ],
   "source": [
    "alpha = 10**(-3)\n",
    "P_S1 = np.mean(train_y)\n",
    "P_S0 = 1 - P_S1\n",
    "log_S1 = np.log(P_S1)\n",
    "log_S0 = np.log(P_S0)\n",
    "\n",
    "tot_S1 = (train_X[train_y == 1]).shape[0]\n",
    "W1_S1 = (np.sum(train_X[train_y == 1], axis = 0) + alpha)  / (tot_S1 + 2*alpha)\n",
    "log_W1_S1 = np.log(W1_S1)\n",
    "\n",
    "tot_S0 = (train_X[train_y == 0]).shape[0]\n",
    "W1_S0 = (np.sum(train_X[train_y == 0], axis = 0)+ alpha)  / (tot_S0 + 2*alpha)\n",
    "log_W1_S0 = np.log(W1_S0)\n",
    "\n",
    "test, indx = np.where((log_W1_S1 - log_W1_S0) > 11)\n",
    "\n",
    "print(vocabulary[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37027369-e144-47bb-a112-36594a6ed9a7",
   "metadata": {},
   "source": [
    "The words that are the best predictors that an email is spam are listed above. This makes sense because most of these words are those that will not appear often in professional email. For example, words like 'capitalfm', 'hottest', 'mlm', 'paste', 'relax', 'spam', 'stealth', and 'yahoo' are words that you would not expect coming from a professional email. The rest of the words are words that might be used to get the reader's attention quickly. For example, words like 'fantastic' and 'bonus' are words that might trick the reader into opening the email because they might think the email has something that they can gain from. Other words, like 'debt' can be used to instill fear to the reader so that they will open the email as soon as possible.\n",
    "\n",
    "The reason why we use a large difference between log_W1_S1 and log_W0_S1 is because a good predictor of spam is, not only when the probability of spam is high (and therefore our fitting function would predict it as a spam email), but also when the probability of it being non-spam is low (therefore our fitting function should never predict this as a spam email)). This is especially important, as it prevents words such as \"and\" that could appear just as frequently in spam and no-spam emails and therefore possibly have very similar probabilites that could change the prediction from no-spam to spam if we had a different dataset that had \"and\" more frequently in spam emails. The vice versa should be true for words that are ideal predictors for no-spam (i.e, when Pr(S = 0|W = 1) − log Pr(S = 1|W = 1) is large). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276af2d9-2fda-40e1-b0ec-6d05a92c7cb0",
   "metadata": {},
   "source": [
    "### 2. Find 10 best words to predict spam and 10 best words to predict non-spam. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93b04dc0-bdc6-4c0d-9eff-1d6ed3d8c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 best words to predict spam: [['bonus' 'profits' 'earning' 'mlm' 'fantastic' 'hottest' 'spam' 'resell'\n",
      "  'unsubscribe' 'relax']]\n",
      "10 best words to predict non-spam: [['linguistic' 'theory' 'syntax' 'abstract' 'deadline' 'grammar'\n",
      "  'committee' 'structure' 'abstracts' 'workshop']]\n"
     ]
    }
   ],
   "source": [
    "alpha = 10**(-3)\n",
    "P_S1 = np.mean(train_y)\n",
    "P_S0 = 1 - P_S1\n",
    "log_S1 = np.log(P_S1)\n",
    "log_S0 = np.log(P_S0)\n",
    "\n",
    "tot_S1 = (train_X[train_y == 1]).shape[0]\n",
    "W1_S1 = (np.sum(train_X[train_y == 1], axis = 0) + alpha)  / (tot_S1 + 2*alpha)\n",
    "log_W1_S1 = np.log(W1_S1)\n",
    "\n",
    "tot_S0 = (train_X[train_y == 0]).shape[0]\n",
    "W1_S0 = (np.sum(train_X[train_y == 0], axis = 0)+ alpha)  / (tot_S0 + 2*alpha)\n",
    "log_W1_S0 = np.log(W1_S0)\n",
    "\n",
    "list_spam = (log_W1_S1 - log_W1_S0)\n",
    "list_no_spam = (log_W1_S0 - log_W1_S1)\n",
    "\n",
    "indx_spam = (-list_spam).argsort()[:,:10]\n",
    "indx_no_spam = (-list_no_spam).argsort()[:,:10]\n",
    "\n",
    "print(\"10 best words to predict spam:\", vocabulary[indx_spam])\n",
    "print(\"10 best words to predict non-spam:\", vocabulary[indx_no_spam])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
